{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27ef2660",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Username</th>\n",
       "      <th>Location</th>\n",
       "      <th>Favorites</th>\n",
       "      <th>Retweets</th>\n",
       "      <th>Followers</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2022-01-01 23:50:17+00:00</th>\n",
       "      <td>1477426984401657857</td>\n",
       "      <td>â€œ#Bitcoin solves a ~$300 trillion problem, and...</td>\n",
       "      <td>BitcoinSapiens</td>\n",
       "      <td>21 Million Citadels</td>\n",
       "      <td>1908</td>\n",
       "      <td>249</td>\n",
       "      <td>16180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-01 23:24:21+00:00</th>\n",
       "      <td>1477420461122375683</td>\n",
       "      <td>Some statistics to start the year:\\n\\nDuring 2...</td>\n",
       "      <td>DigiEconomist</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50793</td>\n",
       "      <td>18615</td>\n",
       "      <td>14240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-01 22:23:31+00:00</th>\n",
       "      <td>1477405149148688389</td>\n",
       "      <td>Asset prices go parabolic and your pay stays s...</td>\n",
       "      <td>thelevelupexp</td>\n",
       "      <td>Orange Coin</td>\n",
       "      <td>1289</td>\n",
       "      <td>229</td>\n",
       "      <td>4722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-01 22:14:00+00:00</th>\n",
       "      <td>1477402754234601479</td>\n",
       "      <td>1. In 2013, I wrote this piece on \"how Bitcoin...</td>\n",
       "      <td>VitalikButerin</td>\n",
       "      <td>Earth</td>\n",
       "      <td>3294</td>\n",
       "      <td>307</td>\n",
       "      <td>4155547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-01 22:05:33+00:00</th>\n",
       "      <td>1477400628573859840</td>\n",
       "      <td>Donâ€™t tell me #Bitcoin wealth is centralized w...</td>\n",
       "      <td>Dennis_Porter_</td>\n",
       "      <td>Oregon, USA</td>\n",
       "      <td>3016</td>\n",
       "      <td>330</td>\n",
       "      <td>115305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-05-31 13:53:38+00:00</th>\n",
       "      <td>1531635015364202496</td>\n",
       "      <td>ðŸŒ¸ $SHIB ðŸŒ¸\\nðŸŒ¸ $LEASH ðŸŒ¸\\nðŸŒ¸ $BONE ðŸŒ¸\\nðŸŒ¸ #Shiboshis...</td>\n",
       "      <td>shibainuart</td>\n",
       "      <td>ðŸŒŒ</td>\n",
       "      <td>2003</td>\n",
       "      <td>511</td>\n",
       "      <td>335951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-05-31 13:53:38+00:00</th>\n",
       "      <td>1531635015364202496</td>\n",
       "      <td>ðŸŒ¸ $SHIB ðŸŒ¸\\nðŸŒ¸ $LEASH ðŸŒ¸\\nðŸŒ¸ $BONE ðŸŒ¸\\nðŸŒ¸ #Shiboshis...</td>\n",
       "      <td>shibainuart</td>\n",
       "      <td>ðŸŒŒ</td>\n",
       "      <td>2004</td>\n",
       "      <td>511</td>\n",
       "      <td>335949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-05-31 10:25:10+00:00</th>\n",
       "      <td>1531582551407591425</td>\n",
       "      <td>$200 | Ends in 24 Hours\\n\\n($100) \\nðŸŸ© RT &amp;amp;...</td>\n",
       "      <td>TheRylai</td>\n",
       "      <td>Dota</td>\n",
       "      <td>1024</td>\n",
       "      <td>4950</td>\n",
       "      <td>557594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-05-31 10:00:02+00:00</th>\n",
       "      <td>1531576225344389121</td>\n",
       "      <td>Thanks to the Wallace-Bolyai-Gerwien Theorem w...</td>\n",
       "      <td>Rainmaker1973</td>\n",
       "      <td>Italy, North by Northwest</td>\n",
       "      <td>1082</td>\n",
       "      <td>219</td>\n",
       "      <td>573897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-05-31 09:25:58+00:00</th>\n",
       "      <td>1531567652111626240</td>\n",
       "      <td>The polygon chain is under maintenance, so it ...</td>\n",
       "      <td>Nexus_Dubai</td>\n",
       "      <td>Polygon Chain</td>\n",
       "      <td>1022</td>\n",
       "      <td>1003</td>\n",
       "      <td>485401</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>34072 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            ID  \\\n",
       "Date                                             \n",
       "2022-01-01 23:50:17+00:00  1477426984401657857   \n",
       "2022-01-01 23:24:21+00:00  1477420461122375683   \n",
       "2022-01-01 22:23:31+00:00  1477405149148688389   \n",
       "2022-01-01 22:14:00+00:00  1477402754234601479   \n",
       "2022-01-01 22:05:33+00:00  1477400628573859840   \n",
       "...                                        ...   \n",
       "2022-05-31 13:53:38+00:00  1531635015364202496   \n",
       "2022-05-31 13:53:38+00:00  1531635015364202496   \n",
       "2022-05-31 10:25:10+00:00  1531582551407591425   \n",
       "2022-05-31 10:00:02+00:00  1531576225344389121   \n",
       "2022-05-31 09:25:58+00:00  1531567652111626240   \n",
       "\n",
       "                                                                       Tweet  \\\n",
       "Date                                                                           \n",
       "2022-01-01 23:50:17+00:00  â€œ#Bitcoin solves a ~$300 trillion problem, and...   \n",
       "2022-01-01 23:24:21+00:00  Some statistics to start the year:\\n\\nDuring 2...   \n",
       "2022-01-01 22:23:31+00:00  Asset prices go parabolic and your pay stays s...   \n",
       "2022-01-01 22:14:00+00:00  1. In 2013, I wrote this piece on \"how Bitcoin...   \n",
       "2022-01-01 22:05:33+00:00  Donâ€™t tell me #Bitcoin wealth is centralized w...   \n",
       "...                                                                      ...   \n",
       "2022-05-31 13:53:38+00:00  ðŸŒ¸ $SHIB ðŸŒ¸\\nðŸŒ¸ $LEASH ðŸŒ¸\\nðŸŒ¸ $BONE ðŸŒ¸\\nðŸŒ¸ #Shiboshis...   \n",
       "2022-05-31 13:53:38+00:00  ðŸŒ¸ $SHIB ðŸŒ¸\\nðŸŒ¸ $LEASH ðŸŒ¸\\nðŸŒ¸ $BONE ðŸŒ¸\\nðŸŒ¸ #Shiboshis...   \n",
       "2022-05-31 10:25:10+00:00  $200 | Ends in 24 Hours\\n\\n($100) \\nðŸŸ© RT &amp;...   \n",
       "2022-05-31 10:00:02+00:00  Thanks to the Wallace-Bolyai-Gerwien Theorem w...   \n",
       "2022-05-31 09:25:58+00:00  The polygon chain is under maintenance, so it ...   \n",
       "\n",
       "                                 Username                   Location  \\\n",
       "Date                                                                   \n",
       "2022-01-01 23:50:17+00:00  BitcoinSapiens        21 Million Citadels   \n",
       "2022-01-01 23:24:21+00:00   DigiEconomist                        NaN   \n",
       "2022-01-01 22:23:31+00:00   thelevelupexp                Orange Coin   \n",
       "2022-01-01 22:14:00+00:00  VitalikButerin                      Earth   \n",
       "2022-01-01 22:05:33+00:00  Dennis_Porter_                Oregon, USA   \n",
       "...                                   ...                        ...   \n",
       "2022-05-31 13:53:38+00:00     shibainuart                          ðŸŒŒ   \n",
       "2022-05-31 13:53:38+00:00     shibainuart                          ðŸŒŒ   \n",
       "2022-05-31 10:25:10+00:00        TheRylai                       Dota   \n",
       "2022-05-31 10:00:02+00:00   Rainmaker1973  Italy, North by Northwest   \n",
       "2022-05-31 09:25:58+00:00     Nexus_Dubai              Polygon Chain   \n",
       "\n",
       "                           Favorites  Retweets  Followers  \n",
       "Date                                                       \n",
       "2022-01-01 23:50:17+00:00       1908       249      16180  \n",
       "2022-01-01 23:24:21+00:00      50793     18615      14240  \n",
       "2022-01-01 22:23:31+00:00       1289       229       4722  \n",
       "2022-01-01 22:14:00+00:00       3294       307    4155547  \n",
       "2022-01-01 22:05:33+00:00       3016       330     115305  \n",
       "...                              ...       ...        ...  \n",
       "2022-05-31 13:53:38+00:00       2003       511     335951  \n",
       "2022-05-31 13:53:38+00:00       2004       511     335949  \n",
       "2022-05-31 10:25:10+00:00       1024      4950     557594  \n",
       "2022-05-31 10:00:02+00:00       1082       219     573897  \n",
       "2022-05-31 09:25:58+00:00       1022      1003     485401  \n",
       "\n",
       "[34072 rows x 7 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "df = pd.read_csv('Crypto_Tweets.csv')\n",
    "df.set_index('Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "652df5dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1477067252784586756    0\n",
      "1477067263031271426    0\n",
      "1477067835503435782    0\n",
      "1477077795108835329    0\n",
      "1477078793810501641    0\n",
      "                      ..\n",
      "1531771109602799617    0\n",
      "1531775365219520512    0\n",
      "1531777170808479744    0\n",
      "1531779319059603457    0\n",
      "1531781252315066369    0\n",
      "Name: ID, Length: 28017, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Date          0.00000\n",
       "ID            0.00000\n",
       "Tweet         0.00000\n",
       "Username      0.00000\n",
       "Location     22.76356\n",
       "Favorites     0.00000\n",
       "Retweets      0.00000\n",
       "Followers     0.00000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#drop duplicate product listings \n",
    "df1 = df.drop_duplicates(subset = ['ID'])\n",
    "\n",
    "#values dropped from each class\n",
    "print(df['ID'].value_counts() - df1['ID'].value_counts())\n",
    "\n",
    "# missing values \n",
    "df1.isnull().sum() * 100 / df.shape[0]\n",
    "\n",
    "##  missing values only in the  loation column "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca70c1b5",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "- I create a preprocessing function for the text data where:\n",
    "    - special characters removed -> noise reduction for the data\n",
    "    - Stopwords removed -> noise reduction for the data\n",
    "    - word tokenization -> to vectors\n",
    "    - words lemmatized -> retains original meaning compared to stemming methods\n",
    "- After I implement a term frequency-inverse document frequency (tf-idf) preprocessor to weight the relative importance of a word in each document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b968498",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\hcho1\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hcho1\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "import nltk \n",
    "\n",
    "#lemmatize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#stopwords set \n",
    "stp = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd7a6098",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize words\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "##remove text\n",
    "\n",
    "def preprocess_text(corpus):\n",
    "    \n",
    "    #basic text cleaning\n",
    "    corpus = str(corpus) #convert all values to string \n",
    "    corpus = corpus.lower() #set text to lowercase \n",
    "    \n",
    "    #remove specific values from text \n",
    "    remove_sym = re.compile('[^0-9a-z #+_]') #compiling symbols that are numeric/special to remove from text \n",
    "    corpus = remove_sym.sub('', corpus)\n",
    "    \n",
    "    #add spaces\n",
    "    space = re.compile('[/(){}\\[\\]\\\\@;]')\n",
    "    corpus = space.sub(' ', corpus)\n",
    "    \n",
    "    #tokenize\n",
    "    tk = RegexpTokenizer(r'\\w+')\n",
    "    corpus = tk.tokenize(corpus)\n",
    "    \n",
    "    #remove stop words\n",
    "    corpus = [i for i in corpus if not i in stp]\n",
    "    \n",
    "    #lemmatize text \n",
    "    lemm_text = [lemma.lemmatize(i) for i in corpus]\n",
    "    corpus = \" \".join(lemm_text)\n",
    "\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b9666c",
   "metadata": {},
   "source": [
    "### Rationale for params\n",
    "- Want to account for unigrams through trigrams \n",
    "- Set importance for each word; i.e. word must appear at least three times in order to be considered in the model \n",
    "- Apply sublinear scaling for document frequency metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61cd0157",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
